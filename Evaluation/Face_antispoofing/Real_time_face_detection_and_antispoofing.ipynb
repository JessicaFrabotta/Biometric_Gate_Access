{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7388565-4a2a-4149-8d64-43e6eec66e16",
   "metadata": {},
   "source": [
    "## Real time face detection and antispoofing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ac632-b177-466e-b629-110d63cb4162",
   "metadata": {},
   "source": [
    "This code implements a real-time face anti-spoofing system using face detection and machine learning techniques. It combines several modules, including OpenCV for video capture, FaceNet-PyTorch (MTCNN) for face detection, and the pre-trained MobileNetV2 model (we trained the last layer of the model on the datasets MSU-MFSD Photos, NUAA and on their combination) to classify faces as \"real\" or \"spoof\".\n",
    "\n",
    "**1. Face Detection**: the code uses the MTCNN module from the facenet_pytorch library to detect faces in real-time from a webcam feed. Once a face is detected, the corresponding region is cropped from the video frame for further analysis.\n",
    "\n",
    "**2. Image Preprocessing**: the detected face is preprocessed before feeding it to the deep learning model. A specific technique, Local Binary Patterns (LBP), is applied optionally to enhance the texture features of the face image, which can help in spoofing detection.\n",
    "\n",
    "**3. Model Loading and Prediction**: the MobileNetV2 model is loaded, and its final layer is modified to output a binary classification for spoof detection. The preprocessed image is passed through the model, which outputs a score that determines whether the face is classified as \"real\" or \"fake.\"\n",
    "\n",
    "**4. Real-Time Detection Loop**: a continuous loop captures frames from the webcam, detects faces, and performs spoofing detection on each detected face. The result is visualized by drawing a rectangle around the detected face and labeling it as either \"Real\" (green) or \"Fake\" (red). The loop continues until the user interrupts the program by pressing 'q'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eee3a8f-b49b-49dd-888d-2f2c5a367d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import warnings\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b5f2bd-2eff-41c9-8051-3cd6d8191870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f569770c-2aba-4f63-80f3-907fb88fd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the Local Binary Pattern (LBP) feature extraction\n",
    "radius = 1 # Radius for LBP calculation\n",
    "n_points = 8 * radius # Number of points for LBP in a circular neighborhood\n",
    "\n",
    "# Function to apply LBP on a single channel (R, G or B) of the image\n",
    "def apply_lbp(channel):\n",
    "    return local_binary_pattern(channel, n_points, radius, method='uniform')\n",
    "\n",
    "# Function to apply LBP transformation to the entire RGB image\n",
    "def lbp_transform(image):\n",
    "    # Apply LBP to each color channel (R, G, B)\n",
    "    lbp_r = apply_lbp(image[:, :, 0])\n",
    "    lbp_g = apply_lbp(image[:, :, 1])\n",
    "    lbp_b = apply_lbp(image[:, :, 2])\n",
    "    # Normalize each channel to a range of 0-255 for display purposes\n",
    "    lbp_r_normalized = cv2.normalize(lbp_r, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    lbp_g_normalized = cv2.normalize(lbp_g, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    lbp_b_normalized = cv2.normalize(lbp_b, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    # Merge the LBP channels back into a color image\n",
    "    lbp_color = cv2.merge((lbp_r_normalized, lbp_g_normalized, lbp_b_normalized))\n",
    "    return lbp_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c059c8-141c-48af-8b70-c7ea92f8aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pre-trained MobileNetV2 model (trained on MSU-MFSD Photos, NUAA and on their combination) and modify it for binary classification\n",
    "def load_trained_mobilenet_model(model_path, device):\n",
    "    loaded_model = models.mobilenet_v2(pretrained=False)\n",
    "    loaded_model.classifier[1] = nn.Linear(loaded_model.classifier[1].in_features, 1)      \n",
    "    loaded_model.load_state_dict(torch.load(model_path)) \n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1861d63-a48d-4e8d-aab3-d4f817a78282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(), # Convert image to PIL format\n",
    "        transforms.Resize(target_size), # Resize the image to the target size (224x224)\n",
    "        transforms.ToTensor(), # Convert image to tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize\n",
    "    ])\n",
    "    image = transform(image) # Apply transformations to the image\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3bb60d2-17ec-453e-b24a-963db7c64ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict whether a face is \"real\" or \"fake\" using the loaded model\n",
    "def detect_real_or_fake(image, model, target_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # Disable gradient computation for faster inference\n",
    "        processed_image = preprocess_image(image, target_size).to(device) # Preprocess and move image to the device\n",
    "        output = model(processed_image) # Run the image through the model\n",
    "        prediction = torch.sigmoid(output).item() # Get the prediction (applying sigmoid for binary classification)\n",
    "        return \"Fake\" if prediction > 0.5 else \"Real\" # Classify based on the output value with respect to the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03b45ce-d019-488e-9623-b156a883f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run face detection and anti-spoofing in real-time\n",
    "def localize_and_detect_spoof_real_time(loaded_model, additional_preprocessing=None):\n",
    "    # Initialize MTCNN for face detection\n",
    "    mtcnn = MTCNN(keep_all=False)\n",
    "    IMG_SIZE = (224, 224) # Target size for the input images to the model\n",
    "    cap = cv2.VideoCapture(0 + cv2.CAP_DSHOW)\n",
    "    face_cropped_window_open = False \n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert the frame to RGB format for face detection\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "        boxes, _ = mtcnn.detect(frame_pil) # Detect faces in the frame\n",
    "        \n",
    "        if boxes is not None: # If any face is detected\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = [int(coord) for coord in box] # Get coordinates of the bounding box\n",
    "                x1, y1 = max(x1, 0), max(y1, 0)\n",
    "                x2, y2 = min(x2, frame.shape[1]), min(y2, frame.shape[0])\n",
    "                \n",
    "                if x1 < x2 and y1 < y2:\n",
    "                    face = frame[y1:y2, x1:x2] # Crop the face region from the frame\n",
    "    \n",
    "                    if face.size > 0:\n",
    "                        face = cv2.resize(face, IMG_SIZE)  # Resize the face to match model input size (224x224)\n",
    "                        if additional_preprocessing == 'lbp':\n",
    "                            face = lbp_transform(face) # Apply LBP if specified\n",
    "    \n",
    "                        result = detect_real_or_fake(face, loaded_model, IMG_SIZE) # Predict whether the face is real or fake\n",
    "                        # Draw a rectangle around the face with color based on the result (green for real, red for fake)\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0) if result == \"Real\" else (0, 0, 255), 2)\n",
    "                        cv2.putText(frame, result, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0) if result == \"Real\" else (0, 0, 255), 2)\n",
    "    \n",
    "                        if not face_cropped_window_open:\n",
    "                            face_cropped_window_open = True\n",
    "                        cv2.imshow('Face cropped', face)\n",
    "                    else:\n",
    "                        if face_cropped_window_open:\n",
    "                            cv2.destroyWindow('Face cropped')\n",
    "                            face_cropped_window_open = False\n",
    "        else:\n",
    "            if face_cropped_window_open:\n",
    "                cv2.destroyWindow('Face cropped')\n",
    "                face_cropped_window_open = False\n",
    "    \n",
    "        cv2.imshow('Face Anti-Spoofing', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11a19e1-30d9-4c58-b8ae-e086061a443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_trained_mobilenet_model(r\"C:\\Users\\Asus\\Desktop\\Progetto_BS_Frabotta_Ferrone\\Evaluation\\Face_antispoofing\\Models\\mobilenet_v2_combined_model.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b04202e-24f1-4dff-ba00-89087ae351b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "localize_and_detect_spoof_real_time(loaded_model, additional_preprocessing=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
